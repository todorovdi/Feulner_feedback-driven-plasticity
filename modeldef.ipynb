{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Toolbox for model definition.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import create_data_velocity_centeroutreach, \\\n",
    "                    create_data_velocity_random, \\\n",
    "                        perturb\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the base RNN Class\n",
    "class RNN(nn.Module):\n",
    "    # base function for the RNN model\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_neurons, \n",
    "                 alpha,\n",
    "                 dtype,\n",
    "                 dt,\n",
    "                 fwd_delay,\n",
    "                 fb_delay=0,\n",
    "                 biolearning=False,\n",
    "                 noiseout = 0,\n",
    "                 noisein = 0,\n",
    "                 nonlin = \"relu\",\n",
    "                 fb_sparsity=1,\n",
    "                 noise_kernel_size=1,\n",
    "                 rec_sparsity=1\n",
    "                ):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        ## Input out size and parameters\n",
    "        self.n_neurons = n_neurons #400\n",
    "        self.n_inputs = n_inputs #3\n",
    "        self.n_outputs = n_outputs #2\n",
    "        self.alpha = alpha #0.20\n",
    "        self.dt = dt #0.01\n",
    "        \n",
    "        # Model Architecture\n",
    "        self.rnn = nn.RNN(n_inputs, n_neurons, num_layers=1, bias=False) #(3,400,1)\n",
    "        self.output = nn.Linear(n_neurons, n_outputs) #(400,2)\n",
    "        self.feedback = nn.Linear(n_outputs, n_neurons) #(2,400)\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Declatration of delays\n",
    "        self.fwd_delay = fwd_delay # delay from neural activity to movement execution, 2\n",
    "        self.fb_delay = fb_delay # feedback delay 10\n",
    "        self.delay = fwd_delay+fb_delay # total delay, 10+2 = 12\n",
    "        \n",
    "        #choosing biolearning\n",
    "        self.biolearning = biolearning  \n",
    "\n",
    "        # if biolearning is true, then the recurrent weights (rnn.weight_hh_l0) are not directly trained by backpropagation\n",
    "        if biolearning:\n",
    "            self.rnn.weight_hh_l0.requires_grad = False\n",
    "\n",
    "        \n",
    "        self.noisein = noisein # amplitute of noise added to input, 0\n",
    "        self.noiseout = noiseout  # amplitude of noise added to output velocity, 0 \n",
    "        self.noise_kernel_lenk = noise_kernel_size # kernel size 5. Convolution used?\n",
    "        \n",
    "        # choosing nonlienearity to be used \n",
    "        if nonlin=='relu':\n",
    "            self.nonlin = torch.nn.ReLU()\n",
    "        elif nonlin=='tanh':\n",
    "            self.nonlin = torch.nn.Tanh()\n",
    "        elif nonlin=='sigmoid':\n",
    "            self.nonlin = torch.nn.Sigmoid()\n",
    "\n",
    "        # mask for feedback weights, zeros or ones\n",
    "        # depends on fb_sparsity, determines sparsity of feedback connections\n",
    "        # it is a non trainable parameter is grad is false\n",
    "        self.mask = nn.Linear(n_outputs, n_neurons, bias=False) # (2,400) \n",
    "        self.mask.weight = nn.Parameter((torch.rand(n_neurons, n_outputs) < fb_sparsity).float()).type(dtype)\n",
    "        self.mask.weight.requires_grad = False\n",
    "\n",
    "        # mask for recurrent weights, zeros or ones\n",
    "        # depends on rec_sparsity, determines sparsity of recurrent connections\n",
    "        # it is a non trainable parameter as grad is false\n",
    "        self.mask2 = nn.Linear(n_neurons, n_neurons, bias=False) \n",
    "        self.mask2.weight = nn.Parameter((torch.rand(n_neurons, n_neurons) < rec_sparsity).float()).type(dtype)\n",
    "        self.mask2.weight.requires_grad = False\n",
    "\n",
    "    # function for initializing the hidden state\n",
    "    def init_hidden(self):\n",
    "        # if batch size 20, neurons 400 then it gives a \n",
    "        # 20 x 400 shape tensor. Each row has 400 values \n",
    "        # between (-0.1, 0.1). Used to initialize the hidden state\n",
    "        return ((torch.rand(self.batch_size, self.n_neurons)-0.5)*0.2).type(self.dtype)\n",
    "    \n",
    "    # function for simulation only 1 time step out of 300 time steps\n",
    "    def f_step(self,\n",
    "               whole_input,\n",
    "               current_net_input,\n",
    "               current_hidden_activation,\n",
    "               velocity_error_feedback,\n",
    "               current_velocity,\n",
    "               external_velocity_perturbation,\n",
    "               popto):\n",
    "        '''\n",
    "        simulates one time bin step (a sub-part of a trial)\n",
    "        Meaning in a trial if there are 300 time steps, this function\n",
    "        simulates 1 time step until it iterates over 300 time steps\n",
    "\n",
    "        in all of the arguments shape[0] = batch_size\n",
    "\n",
    "        whole_input =  input shape torch.Size([20, 7])\n",
    "        current_net_input  =  current hidden net input (pre-activation)   torch.Size([20, 400])\n",
    "        current_hidden_activation  =  current hidden activation (applied on current net input)  torch.Size([20, 400])\n",
    "        velocity_error_feedback = current feedback (2D coordintes of the error torch.Size([20, 2])\n",
    "        current_velocity = current velocity(?)  shape torch.Size([20, 2])\n",
    "        external_velocity_perturbation  = external perturbation applied to velocity shape (20,2)\n",
    "        pin  pertrubation of velocity in current timebin shape = (20, 2)\n",
    "        popto perturbation of the neural activity (pre-synaptic) shape torch.Size([20, 400])\n",
    "\n",
    "        returns current_net_input (neural activity),current_hidden_activation, current_velocity \n",
    "            current_net_input shape is (20, 400)\n",
    "            current_hidden_activation torch.Size([20, 400])\n",
    "            current_velocity  = will be saved in poserr, shape=torch.Size([20, 2])\n",
    "        '''\n",
    "\n",
    "        # whole_input is the input from the \"stiumulus\" dataset\n",
    "        # whole_input col 0,1 = (endpoint_x - startpoint_x), (endpoint_y - startpoint_y)\n",
    "        # whole_input col 2 = stim range flag. Only active until go_on signal occurs\n",
    "        # whole_input col 3,4 = the \"perfect\" velocity_x and velocity_y to reach endpoint\n",
    "        # whole_input col 5,6 = the \"perfect\" trajectory of x,y coordinate moves to reach endpoint\n",
    "          \n",
    "        # whole_input col [0-1] is zero before stim_on, after stim_on it is end_point-start_point\n",
    "        # whole_input col 2  before go_on[k]-go_to_peak = 0, after = stim_range (hold signal)\n",
    "\n",
    "        # perceived signals are the first 3 columns of the input\n",
    "        perceived_signals = whole_input[:,:3]\n",
    "\n",
    "        # FORMULA:\n",
    "        # 1.    - sparse rnn hidden weights with mask2 matrix, then transpose it\n",
    "        #       - matmul the transposed hidden weight matrix with current hidden activation\n",
    "\n",
    "        # 2.    - Transpose input hidden weight matrix\n",
    "        #       - Matmul hiddden weight matrix with first 3 cols of \"stimulus\"\n",
    "\n",
    "        # 3.    - Sparse the feedback weight matrix with mask1 matrix\n",
    "        #       - Matmul the sparsed feedback weight matrix with current velocity error matrix\n",
    "\n",
    "        # 4.    - (1+2+3+ bias of feedback + velocity perturbation - current hidden net input)\n",
    "        # 5.    - Multiply 4 with alpha, then add current hidden net input \n",
    "        current_net_input = current_net_input + self.alpha*(-current_net_input + current_hidden_activation @ (self.mask2.weight*self.rnn.weight_hh_l0).T\n",
    "                                  + perceived_signals @ self.rnn.weight_ih_l0.T\n",
    "                                  + velocity_error_feedback @ (self.mask.weight*self.feedback.weight).T \n",
    "                                  + self.feedback.bias.T\n",
    "                                  + popto\n",
    "                              )\n",
    "        \n",
    "        # 6.    - Apply nonlinearity to (5)\n",
    "        current_hidden_activation = self.nonlin(current_net_input)\n",
    "\n",
    "        # 7.    - Decode the velocity using the output layer, add velocity perturbation to decoded velocity\n",
    "        vt = self.output(current_hidden_activation) + external_velocity_perturbation # vt.shape torch.Size([20, 2])\n",
    "\n",
    "\n",
    "        # xin[:,3:5] = last two dimensions of target mvt (so velolcity)\n",
    "        # current_velocity is the accumulated (signed) mismatch between the target and the actual velocity\n",
    "\n",
    "        # 8. Calculate current velocity error, multiply with time step, add to 7 (\"produced\" velocity)\n",
    "        updated_velocity = current_velocity + self.dt*(whole_input[:,3:5]-vt) # will be added to poserr \n",
    "\n",
    "        return current_net_input,current_hidden_activation, updated_velocity\n",
    "    \n",
    "    # Function for biologically possible learning rule\n",
    "    def dW(self,pre,post,lr,inp,fb,presum):\n",
    "        with torch.no_grad():\n",
    "            return self.alpha*0.1*(\n",
    "                + lr*self.mask2.weight*torch.outer(fb@(self.mask.weight*self.feedback.weight).T,presum)\n",
    "                )\n",
    "                    \n",
    "    # function for getting velocity output\n",
    "    def get_output(self,testl1):\n",
    "        # what is the test1 parameter?\n",
    "        # is testl1 of (shape 20x400)?\n",
    "        if self.noiseout>0:\n",
    "            return self.output(testl1)+self.output(testl1)*self.create_noise(testl1)\n",
    "        else:\n",
    "            return self.output(testl1)\n",
    "        \n",
    "    # function for simulation motor noise (scales with velocity output)\n",
    "    def create_noise(self,testl1):\n",
    "        '''\n",
    "        testl1 is a 2D tensor, it is not actually used, only its shape is used\n",
    "        '''\n",
    "        # time varying noise\n",
    "        tmp = self.noiseout*torch.randn(testl1.shape[1],testl1.shape[0],2)\n",
    "        tmp = tmp.permute(0,2,1)\n",
    "        lenk = self.noise_kernel_lenk\n",
    "        kernel = torch.ones(2,tmp.shape[1],lenk)/lenk\n",
    "        padding = lenk // 2 + lenk % 2\n",
    "\n",
    "        # noise smoothing with 1D convolution\n",
    "        noise = torch.nn.functional.conv1d(tmp, kernel, padding=padding)[:,:,:testl1.shape[0]]\n",
    "        noise = noise.permute(2,0,1)*np.sqrt(lenk)  \n",
    "        return noise\n",
    "    \n",
    "     # the main forward pass function \n",
    "    def forward(self, Stimulus, Velocity_pert, lr=1e-3, popto=None):\n",
    "        '''\n",
    "        Stimulus is \"stimulus\" or \"input\" or \"whole_input\" as seen in f_step, \n",
    "        Velocity_pert is velocity perturbation\n",
    "        \n",
    "        Stimulus.shape = 300x20x7  = time x batch x input_dim\n",
    "        Velocity_pert.shape = 300x20x2 = velocity pertrubations \n",
    "        \n",
    "        popto = perturbation of the neural activty (pre synaptic), used in adaptation_learning.py only\n",
    "        \n",
    "        Each forward run is simulation of the entire single trial, meaning over 300 times steps \n",
    "        '''\n",
    "\n",
    "        # INITINALIZING THE HIDDEN STATE\n",
    "        self.batch_size = Stimulus.size(1) # declaring batch size as 20 \n",
    "        hidden0 = self.init_hidden() # making a hidden leyaer and initializing with random numbers\n",
    "        \n",
    "        current_net_input = hidden0         # random values between (-0.1, 0.1), shape is (20 x 400) \n",
    "        current_hidden_activation = self.nonlin(current_net_input) # initial activation function, likely relu shape = batch x n_neurons (20x400)\n",
    "        current_velocity_output = self.output(current_hidden_activation) # initial velocity outputs, shape = batch x n_output (20 x 2)\n",
    "        \n",
    "        presum = current_hidden_activation # is initial hidden activation, just stored in presum for later calculation\n",
    "        dw_acc = torch.zeros(self.rnn.weight_hh_l0.shape) # initialized as an accummulator for changes in the recurrent weights 400 x 400\n",
    "        \n",
    "        # Helper arrays to store some values\n",
    "        hidden1 = [] #initializing empty list to store hidden activation at each time step\n",
    "        position_error = [] # List to store accummulated velocity error at each time step\n",
    "\n",
    "\n",
    "        ######## SKIP FOR NOW #######\n",
    "        if popto is None:\n",
    "            popto = torch.zeros((Stimulus.shape[0],Stimulus.shape[1],current_hidden_activation.shape[1])) # 300 x batch x 400\n",
    "\n",
    "\n",
    "        # prerun simulation (until 'delay' is reached)\n",
    "        for j in range(self.fwd_delay+1):\n",
    "            current_net_input,current_hidden_activation,current_velocity_output = self.f_step(Stimulus[0],\n",
    "                                   current_net_input,      #current hidden net input\n",
    "                                   current_hidden_activation,      #current hidden net activation \n",
    "                                   current_velocity_output*0,    #velocity error feedback, set to 0\n",
    "                                   current_velocity_output,      #current velocity\n",
    "                                   Velocity_pert[0], #current velocity perurbation\n",
    "                                   popto[0]), #neural activity perturbation \n",
    "            # current_velocity_output is the accumulated (signed) mismatch between the target and the actual velocity\n",
    "            hidden1.append(current_hidden_activation)\n",
    "            position_error.append(current_velocity_output)\n",
    "\n",
    "        # generate noise on the input level (input and output)\n",
    "        Stimulus = Stimulus+ self.noisein*torch.randn(Stimulus.shape[1],Stimulus.shape[2])[None,:,:]\n",
    "\n",
    "        # noise applied to the output velocity\n",
    "        if self.noiseout>0:\n",
    "            noise = self.create_noise(Velocity_pert)\n",
    "        else:\n",
    "            noise = torch.zeros(Velocity_pert.shape)\n",
    "\n",
    "        # now real time simulation, it will loop 300 times\n",
    "        # meaning an entire trial is simulated \n",
    "        for j in range(Stimulus.size(0)):\n",
    "\n",
    "            # creating tuple containing the input at the current time step\n",
    "            # as well as the current hidden states (current net,current hidden activated) for f_step function\n",
    "            tpl = Stimulus[j],current_net_input,current_hidden_activation\n",
    "\n",
    "            # creating a dictionary for additional arguments for f_step function\n",
    "            d = dict(v1=current_velocity_output, \n",
    "                     vel_pert_ext=Velocity_pert[j]+noise[j]*self.output(current_hidden_activation), \n",
    "                     popto= popto[j])\n",
    "            \n",
    "            # if feedback delay is 0, or the trial number is less than feedback delay then we do not use feedback\n",
    "            if (self.fb_delay<0) | (j<=self.fb_delay):\n",
    "                current_net_input,current_hidden_activation,current_velocity_output = self.f_step(*tpl,\n",
    "                                                                                                  velocity_error_feedback=current_velocity_output*0, \n",
    "                                                                                                  **d)\n",
    "                \n",
    "            # else we use feedback\n",
    "            else:\n",
    "                feedback_val_curf = position_error[j-self.delay]\n",
    "                current_net_input,current_hidden_activation,current_velocity_output = self.f_step(*tpl,\n",
    "                                                                                     velocity_error_feedback=feedback_val_curf, \n",
    "                                                                                     **d)\n",
    "\n",
    "                if self.biolearning and j%5:\n",
    "                    dw_acc += self.dW(current_net_input[0],current_hidden_activation[0],lr,Stimulus[j,0],\n",
    "                                      position_error[j-self.delay][0],presum[0]).detach()\n",
    "            if self.biolearning:\n",
    "                presum += current_hidden_activation.detach()\n",
    "\n",
    "            # v1 is the accumulated (signed) mismatch between the target and the actual velocity\n",
    "            hidden1.append(current_hidden_activation)\n",
    "            position_error.append(current_velocity_output)\n",
    "\n",
    "        # truncate very first (probably because it was random.. but we ran warmup)\n",
    "        hidden1 = torch.stack(hidden1[1:])\n",
    "        position_error = torch.stack(position_error[1:])\n",
    "\n",
    "        if self.biolearning:\n",
    "            with torch.no_grad():\n",
    "                self.rnn.weight_hh_l0 += dw_acc\n",
    "\n",
    "        # we later do\n",
    "        # output,hidden = model(stimulus[epoch],pert[epoch]) # runs forward\n",
    "        # loss_train = criterion(output, output*0).mean() # we compare output (errors per trial) with 0\n",
    "\n",
    "        if self.fwd_delay==0:\n",
    "            return position_error, hidden1\n",
    "        elif self.fwd_delay>0:\n",
    "            return position_error[(self.fwd_delay):], hidden1[:(-self.fwd_delay)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run and Test Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,params,data,fb=True,dopert=0):\n",
    "    \"\"\"\n",
    "    Runs the specified model on the provided data with optional feedback and perturbation.\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The neural network model to be run.\n",
    "    params (dict): Dictionary containing parameters for the model and perturbation.\n",
    "    data (dict): Dictionary containing the test data with keys 'test_set', 'target', 'stimulus', and optionally 'tids'.\n",
    "    fb (bool, optional): Flag to enable or disable feedback. Defaults to True.\n",
    "    dopert (int, optional): Type of perturbation to apply. 0 for no perturbation, 1 for random pushes. Defaults to 0.\n",
    "    Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "        - 'target': The target data from the test set.\n",
    "        - 'stimulus': The stimulus data from the test set.\n",
    "        - 'error': The error between the model output and the target.\n",
    "        - 'peak_speed': The peak speed data from the test set.\n",
    "        - 'pert': The perturbation applied to the model.\n",
    "        - 'activity': The activity of the model's hidden layer.\n",
    "        - 'output': The output of the model.\n",
    "        - 'tid' (optional): The trial IDs from the test set, if present.\n",
    "    \"\"\"\n",
    "    '''only used in test()'''\n",
    "\n",
    "    if not fb:\n",
    "        state_dict = model.state_dict()\n",
    "        save = state_dict['feedback.weight'].detach().clone()\n",
    "        state_dict['feedback.weight'] *= 0\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    from dataset import perturb\n",
    "\n",
    "    # SETUP ############\n",
    "    testdata = data['test_set']\n",
    "    tout = testdata['target'][:,:,2:]\n",
    "    tstim = testdata['stimulus']\n",
    "    stim = torch.Tensor(tstim.transpose(1,0,2)).type(model.dtype)\n",
    "    pert = torch.zeros(tout.transpose(1,0,2).shape).type(model.dtype)\n",
    "    if dopert==1: # if perturbation type is random pushes\n",
    "        pert = perturb(pert,stim.shape[1],params['p1'])\n",
    "\n",
    "    # RUN ############\n",
    "    testout,testl1 = model(stim,pert)\n",
    "    error = testout.cpu().detach().numpy().transpose(1,0,2)\n",
    "    testout = model.get_output(testl1) + pert\n",
    "\n",
    "    # SAVE ############\n",
    "    output = testout.cpu().detach().numpy().transpose(1,0,2)\n",
    "    activity = testl1.cpu().detach().numpy().transpose(1,0,2)\n",
    "    pert = pert.cpu().detach().numpy().transpose(1,0,2)\n",
    "    dic = {'target':tout,'stimulus':tstim,'error':error,\n",
    "            'peak_speed':testdata['peak_speed'],'pert':pert,\n",
    "            'activity':activity,'output':output}\n",
    "    \n",
    "    if 'tids' in testdata.keys():\n",
    "        dic.update({'tid':testdata['tids']})\n",
    "    if not fb:\n",
    "        state_dict = model.state_dict()\n",
    "        state_dict['feedback.weight'] = save\n",
    "        model.load_state_dict(state_dict)\n",
    "    return dic\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
